{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA docomposition of a general function\n",
    "\n",
    "Functional ANOVA decomposition represents a high-dimensional function as a function of the form\n",
    "$$f(x_1, x_2, \\ldots, x_D) = f_0 + \\sum_{i=1}^D f_i(x_i) + \\sum_{i<j} f_{ij}(x_i, x_j) + \\sum_{i<j<k} f_{ijk}(x_i, x_j, x_k) + \\cdots + f_{1,\\ldots,D}(x_1, \\ldots, x_D).$$\n",
    "\n",
    "\n",
    "Let's first talk about how to compute the ANOVA components in general and then we focus on how to do it for $f(x_1, \\ldots, x_D)$ which is evaluated as a mean of a Gaussian process.\n",
    "\n",
    "Note: without the loss of generality, we assumy that the input is restricted to a hypercube $[0,1]^D$. It will significanlty simplify the notation. We can normalize the input whe computing.\n",
    "\n",
    "The first step is to choose the projection operator $P$:\n",
    "$$Pf := \\int_{[0,1]}f(x)d\\mu(x)$$\n",
    "We are going to use the projection operator $P$ using Lebegue measure $Pf := \\int_{[0,1]} f(x) dx$, so all intergrals should work out as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional case\n",
    "\n",
    "Now we use the projection operator to define the constant and the main effects. We assume $D=2$ for now and then introduce some more notation to generalize:\n",
    "\\begin{align}\n",
    "f_0 & = \\int_{[0, 1]} \\int_{[0,1]} f(x_1, x_2) dx_1 dx_2 \\\\\n",
    "f_1 (x_1) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_2  \\\\\n",
    "f_2 (x_2) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_1  \n",
    "\\end{align}\n",
    "\n",
    "The interaction effect $f_{1,2}(x_1,x_2)$ is defined as the remainder to make the ANOVA decomposition to work out correctly:\n",
    "$$f_{1,2}(x_1,x_2) = f(x_1, x_2) - f_0 - f_1(x_1) - f_2(x_2).$$\n",
    "\n",
    "The **total variance** (TV) of the predictor is defined as\n",
    "$$\\sigma^2(f) := \\int_{[0, 1]} \\int_{[0, 1]} (f(x_1, x_2) - f_0)^2 d x_1 d x_2$$\n",
    "\n",
    "One can show that TV is decomposible into the sum of variances of main effects and interactions defined above:\n",
    "\\begin{align}\n",
    "\\sigma_1^2(f_1) &:= \\int_{[0, 1]} (f_1(x_1))^2 d x_1 \\\\\n",
    "\\sigma_2^2(f_2) &:= \\int_{[0, 1]} (f_2(x_2))^2 d x_2 \\\\\n",
    "\\sigma_{1,2}^2(f_{1,2}) &:= \\int_{[0, 1]} \\int_{[0, 1]} \\left(f_{1,2}(x_1, x_2) - f_0 - \\left[f_1(x_1)-f_0\\right] - \\left[f_2(x_2)-f_0\\right]\\right)^2 dx_1 dx_2 \\\\ \n",
    "\\sigma^2(f) &= \\sigma_1^2(f_1) + \\sigma_2^2(f_2) + \\sigma_{1,2}^2 (f_{1,2}).\n",
    "\\end{align}\n",
    "\n",
    "And so, by dividing individual variances by TV we can express these compoents as percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General case\n",
    "\n",
    "Using subsets $u\\subseteq \\{1,\\ldots, D\\}$, we can establish a shorthand notation for ANOVA components, where $f_u$ and $x_u$ represents a subset of vector $x$ with components $x_i, i \\in u$. The we have\n",
    "\\begin{align}\n",
    "f(x_1, \\ldots, x_D) &= \\sum_{u \\subseteq\\{1\\ldots,D\\}} f_{u}(x_{u}),\\\\\n",
    "f_{u}(x_{u}) &= \\int_{[0,1]^{D-|u|}} \\Bigl(f(x) - \\sum_{v\\subsetneq u}f_v(x_v)\\Bigr)dx_{-u}\\\\\n",
    "\\sigma^2(f_u) &= \\int_{[0,1]^D} \\Bigl(f_u(x_u)\\Bigr)^2dx \\\\\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D} \\Bigl(f(x) - f_0\\Bigr)^2 dx = \\sum_{u \\subseteq \\{1,\\ldots,D\\}, u\\neq\\emptyset}\\sigma^2(f_u).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA for Gaussian process\n",
    "\n",
    "For Bayesian optimization we are using the Gaussian proces defined by a parametrized RBF kernel of the form\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2 \\exp\\Bigl(-(x-x')^t D(\\vec{\\theta}_1) (x-x'))\\Bigr),\n",
    "$$\n",
    "where $[D(\\vec{\\theta}_1)]_{ij} = (\\theta_{1,i}^2 + \\epsilon)\\delta_{ij}$ is a diagonal matrix of scaling coefficients, $\\epsilon$ beeing a constant and $\\delta_{ij}$ beeing Dirac-delta.\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2\\exp\\Bigl(-\\sum_{d=1}^D (\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) = \\theta_0^2 \\prod_{d=1}^D \\exp \\Bigl(-(\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) . \n",
    "$$\n",
    "\n",
    "I drop $\\theta$'s from the definition of $K$ for shorthand.\n",
    "\n",
    "Let $\\Sigma$ be the correlation matrix of the training set $\\{(x_i, y_i)\\}_{i=1}^n$ and $k(x)$ a vector with correlations to the new point $x$, to evaluate the functional given by the GP and get the mean at the points $x$  we do:\n",
    "\\begin{align}\n",
    "\\Sigma &:= \\{K(x_i, x_j)\\}_{ij} \\\\\n",
    "k(x) &:= \\{K(x,x_i)\\}_{i=1}^n \\\\\n",
    "f(x) &:= k(x)^t \\Sigma^{-1}y \\\\\n",
    "&= \\sum_{i=1}^n K(x, x_i) \\Sigma_{i}^{-1}y,\n",
    "\\end{align}\n",
    "where $\\Sigma_i^{-1}$ is the $i$'th row of the inverse correlation matrix (of course, we are just solving $\\Sigma z = y$ and take the $i$'th component of $z$, so I will just use $z_i$ as a shorthand:\n",
    "$$ f(x) = \\sum_{i=1}^n z_i K(x, x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use wolframalpha to derive integrands. We also define a shorthand\n",
    "$$\n",
    "c_d := \\sqrt{\\theta_{1d}^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "For the [constant](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28x-y%29%5E2%29dx+from+0+to+1) we have:\n",
    "\\begin{align}\n",
    "f_0 &= \\int_{[0,1]^D} f(x) dx = \\int_{[0,1]^D} \\sum_{i=1}^n z_i K(x, x_i) dx \\\\\n",
    "&= \\sum_{i=1}^n z_i \\theta_0^2 \\prod_{d=1}^D \\int_{0}^{1} \\exp \\Bigl(-c_d^2(x_d-x_{id})^2\\Bigr) dx_d \\\\\n",
    "&= \\theta_0^2 \\sum_{i=1}^n z_i \\prod_{d=1}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d-c_d x_{id})\\Bigr) + \\text{erf}\\Bigl(c_d x_{id}\\Bigr)\\Bigr]}{2c_d}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the [total variance](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28%28x-y%29%5E2%2B%28x-z%29%5E2%29%29dx+from+a+to+b+) we have:\n",
    "\\begin{align}\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D}\\Bigl(f(x) - f_0\\Bigr)^2 dx = \n",
    "\\int_{[0,1]^D} \\Bigl(\\sum_{i=1}^n z_i K(x, x_i) - f_0 \\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j K(x, x_i) K(x, x_j) dx - 2f_0 \\int_{[0,1]^D}\\sum_{i=1}^n z_i K(x, x_i) dx + f_0^2\\int_{[0,1]^D}dx \\\\\n",
    "&= f_0^2\\Bigl(1 - 2\\Bigr) + \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\int_{0}^{1} \\exp\\Bigl(-c_d^2 \\Bigl[(x_d - x_{id})^2 + (x_d - x_{jd})^2\\Bigr]\\Bigr)dx_d \\\\\n",
    "&= \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\frac{\\sqrt{\\frac{\\pi}{2}} \\exp\\Bigl(-1/2c_d^2(x_{id}-x_{jd})^2\\Bigr)\\Bigl[\\text{erf}\\Bigl(\\frac{c_d(x_{id}+x_{jd})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_d(-2 + x_{id} + x_{jd})}{\\sqrt{2}}\\Bigr)\\Bigr]}{2c_d} -f_0^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main effects we have:\n",
    "\\begin{align}\n",
    "f_t(x_t) &= \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr)\\prod_{d\\neq t}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d(1-x_{id})\\Bigr) - \\text{erf}\\Bigl(c_d(-x_{id})\\Bigr)\\Bigr]}{2c_d} - f_0\\\\ \n",
    "&=: \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - f_0 \\\\\n",
    "\\sigma^2(f_t(x_t)) &= \\int_{[0,1]^D} \\Bigl(f_{d}(x_{d})\\Bigr)^2 dx = \\int_{[0,1]^D} \\Bigl(\\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - f_0\\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\exp\\Bigl(-c_t \\Bigl[(x_t - x_{it})^2 + (x_t - x_{jt})^2\\Bigl]\\Bigl) \\\\ \n",
    "&- 2 \\theta_0^2 f_0 \\sum_{i=1}^n z_i P_i \\int_{[0,1]^D} \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) dx + f_0^2 \\int_{[0,1]^D} dx \\\\\n",
    "&= \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\frac{\\sqrt{\\frac{\\pi}{2}} e^{-1/2c_t^2(x_{it}-x_{jt})^2}\\Bigl[\\text{erf}\\Bigl(\\frac{c_t(x_{it}+x_{jt})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_t(-2 + x_{it} + x_{jt})}{\\sqrt{2}}\\Bigr)\\Bigl]}{2c_t} \\\\\n",
    "&- 2\\theta_0^2 f_0 \\sum_{i=1}^n z_i P_i \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_t(1-x_{it})\\Bigr) + \\text{erf}\\Bigl(c_t x_{it}\\Bigr)\\Bigr]}{2c_t}  + f_0^2 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.integrate import dblquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define GP and use normalization\n",
    "dim = 2\n",
    "n = 10\n",
    "theta_0 = 2.0\n",
    "theta_1 = np.array([0.3]*dim)\n",
    "ε = 1e-8\n",
    "c = np.sqrt(theta_1**2 + ε)\n",
    "length_scale = 1.0/(np.sqrt(2)*c)\n",
    "kernel = RBF(length_scale = length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=n, n_features=dim, n_informative=dim, bias=0.5, random_state=1000)\n",
    "\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "X_train = X_norm\n",
    "y_train = y\n",
    "#X_test = X_norm[10:,:]\n",
    "#y_test = y[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = theta_0**2*pairwise_kernels(X_train, metric=kernel)\n",
    "z = np.linalg.solve(K, y_train)\n",
    "\n",
    "def kernel_eval(X):\n",
    "    return theta_0**2*pairwise_kernels(X, X_train, metric=kernel) @ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contant $f_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.37201083396576"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant analytic\n",
    "f_0 = theta_0**2 * z.T @ np.prod(np.sqrt(np.pi)/(2*c)* (erf(c*(1-X_train)) + erf(c*X_train)), axis=1)\n",
    "f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 6.070411 rel_error = 3.424463\n",
      "       100 value = 0.592990 rel_error = 0.567795\n",
      "      1000 value = 1.548451 rel_error = 0.128600\n",
      "     10000 value = 1.423408 rel_error = 0.037461\n",
      "    100000 value = 1.429667 rel_error = 0.042023\n",
      "   1000000 value = 1.343758 rel_error = 0.020592\n",
      "  10000000 value = 1.373827 rel_error = 0.001324\n"
     ]
    }
   ],
   "source": [
    "# Constant MC\n",
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=2*n).reshape((-1,dim))\n",
    "    sol = np.mean(kernel_eval(X))\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs((sol-f_0)/f_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total variance $\\sigma^2(f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_ij(i, j ):\n",
    "    return z[i] * z[j] * np.prod(np.sqrt(np.pi/2)/(2*c)\n",
    "             * np.exp(-0.5*c**2*(X_train[i,:]- X_train[j, :])**2)\n",
    "            * (erf(c/np.sqrt(2)*(X_train[i,:] + X_train[j, :])) \n",
    "              - erf(c/np.sqrt(2)*(X_train[i,:] + X_train[j, :]-2))) )\n",
    "s = 0\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        s += prod_ij(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666.8540544524947"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma2 = theta_0**4* s - f_0**2\n",
    "sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 684.314962 rel_error = 0.026184\n",
      "       100 value = 813.835617 rel_error = 0.220410\n",
      "      1000 value = 659.688143 rel_error = 0.010746\n",
      "     10000 value = 673.017627 rel_error = 0.009243\n",
      "    100000 value = 666.401597 rel_error = 0.000678\n",
      "   1000000 value = 666.544669 rel_error = 0.000464\n",
      "  10000000 value = 666.535369 rel_error = 0.000478\n"
     ]
    }
   ],
   "source": [
    "# Constant MC\n",
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=2*n).reshape((-1,dim))\n",
    "    sol = np.mean((kernel_eval(X) - f_0)**2)\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs(sol-sigma2)/sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main effects $f_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_d(i, d): \n",
    "    return np.sqrt(np.pi)/(2*c[d])*(erf(c[d]*(1-X_train[i,d])) + erf(c[d]* X_train[i,d])) \n",
    "def P_i(i, t):\n",
    "    p = 1\n",
    "    for d in range(X_train.shape[1]):\n",
    "        if d != t:\n",
    "            p *= prod_d(i, d)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_effect(t):\n",
    "    s1 = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            s1 += P_i(i, t)*P_i(j, t)*prod_ij(i, j)\n",
    "    s1 *= theta_0**4\n",
    "    s2 = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        s2 += z[i] * prod_d(i, t) * P_i(i, t)\n",
    "    s2 *= -2*theta_0**2*f_0\n",
    "    return s1 + s2 + f_0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.44749234771336"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me0 = main_effect(0)\n",
    "me0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_t_mc(x, t, n=10**5):\n",
    "    n = 10**5\n",
    "    X = np.random.uniform(size=n*2).reshape((-1,dim))\n",
    "    X[:,t] = x\n",
    "    return np.mean(kernel_eval(X)-f_0)\n",
    "\n",
    "def f_t(x,t):\n",
    "    s = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        s += z[i]*np.exp(-c[t]**2*(x - X_train[i, t])**2)*P_i(i, t)\n",
    "    return theta_0**2 * s-f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 1.845177 rel_error = 0.985291\n",
      "       100 value = 2.424765 rel_error = 0.980671\n",
      "      1000 value = 2.973315 rel_error = 0.976298\n",
      "     10000 value = 2.895713 rel_error = 0.976917\n",
      "    100000 value = 2.919184 rel_error = 0.976730\n",
      "   1000000 value = 2.914160 rel_error = 0.976770\n",
      "  10000000 value = 2.919527 rel_error = 0.976727\n"
     ]
    }
   ],
   "source": [
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=n)\n",
    "    sol = np.mean((f_t(X, 0))**2)\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs(sol-me0)/me0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2243594681683878, -1.187763547675786)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t_mc(0.3, 0, 10**5), f_t(0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.37201083396576"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

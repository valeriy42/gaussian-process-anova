{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA docomposition of a general function\n",
    "\n",
    "Functional ANOVA decomposition represents a high-dimensional function as a function of the form\n",
    "$$f(x_1, x_2, \\ldots, x_D) = f_0 + \\sum_{i=1}^D f_i(x_i) + \\sum_{i<j} f_{ij}(x_i, x_j) + \\sum_{i<j<k} f_{ijk}(x_i, x_j, x_k) + \\cdots + f_{1,\\ldots,D}(x_1, \\ldots, x_D).$$\n",
    "\n",
    "\n",
    "Let's first talk about how to compute the ANOVA components in general and then we focus on how to do it for $f(x_1, \\ldots, x_D)$ which is evaluated as a mean of a Gaussian process.\n",
    "\n",
    "Note: without the loss of generality, we assumy that the input is restricted to a hypercube $[0,1]^D$. It will significanlty simplify the notation. We can normalize the input whe computing.\n",
    "\n",
    "The first step is to choose the projection operator $P$:\n",
    "$$Pf := \\int_{[0,1]}f(x)d\\mu(x)$$\n",
    "We are going to use the projection operator $P$ using Lebegue measure $Pf := \\int_{[0,1]} f(x) dx$, so all intergrals should work out as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional case\n",
    "\n",
    "Now we use the projection operator to define the constant and the main effects. We assume $D=2$ for now and then introduce some more notation to generalize:\n",
    "\\begin{align}\n",
    "f_0 & = \\int_{[0, 1]} \\int_{[0,1]} f(x_1, x_2) dx_1 dx_2 \\\\\n",
    "f_1 (x_1) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_2  \\\\\n",
    "f_2 (x_2) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_1  \n",
    "\\end{align}\n",
    "\n",
    "The interaction effect $f_{1,2}(x_1,x_2)$ is defined as the remainder to make the ANOVA decomposition to work out correctly:\n",
    "$$f_{1,2}(x_1,x_2) = f(x_1, x_2) - f_0 - f_1(x_1) - f_2(x_2).$$\n",
    "\n",
    "The **total variance** (TV) of the predictor is defined as\n",
    "$$\\sigma^2(f) := \\int_{[0, 1]} \\int_{[0, 1]} (f(x_1, x_2) - f_0)^2 d x_1 d x_2$$\n",
    "\n",
    "One can show that TV is decomposible into the sum of variances of main effects and interactions defined above:\n",
    "\\begin{align}\n",
    "\\sigma_1^2(f_1) &:= \\int_{[0, 1]} (f_1(x_1))^2 d x_1 \\\\\n",
    "\\sigma_2^2(f_2) &:= \\int_{[0, 1]} (f_2(x_2))^2 d x_2 \\\\\n",
    "\\sigma_{1,2}^2(f_{1,2}) &:= \\int_{[0, 1]} \\int_{[0, 1]} \\left(f_{1,2}(x_1, x_2) - f_0 - \\left[f_1(x_1)-f_0\\right] - \\left[f_2(x_2)-f_0\\right]\\right)^2 dx_1 dx_2 \\\\ \n",
    "\\sigma^2(f) &= \\sigma_1^2(f_1) + \\sigma_2^2(f_2) + \\sigma_{1,2}^2 (f_{1,2}).\n",
    "\\end{align}\n",
    "\n",
    "And so, by dividing individual variances by TV we can express these compoents as percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General case\n",
    "\n",
    "Using subsets $u\\subseteq \\{1,\\ldots, D\\}$, we can establish a shorthand notation for ANOVA components, where $f_u$ and $x_u$ represents a subset of vector $x$ with components $x_i, i \\in u$. The we have\n",
    "\\begin{align}\n",
    "f(x_1, \\ldots, x_D) &= \\sum_{u \\subseteq\\{1\\ldots,D\\}} f_{u}(x_{u}),\\\\\n",
    "f_{u}(x_{u}) &= \\int_{[0,1]^{D-|u|}} \\Bigl(f(x) - \\sum_{v\\subsetneq u}f_v(x_v)\\Bigr)dx_{-u}\\\\\n",
    "\\sigma^2(f_u) &= \\int_{[0,1]^D} \\Bigl(f_u(x_u)\\Bigr)^2dx \\\\\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D} \\Bigl(f(x) - f_0\\Bigr)^2 dx = \\sum_{u \\subseteq \\{1,\\ldots,D\\}, u\\neq\\emptyset}\\sigma^2(f_u).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA for Gaussian process\n",
    "\n",
    "For Bayesian optimization we are using the Gaussian proces defined by a parametrized RBF kernel of the form\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2 \\exp\\Bigl(-(x-x')^t D(\\vec{\\theta}_1) (x-x'))\\Bigr),\n",
    "$$\n",
    "where $[D(\\vec{\\theta}_1)]_{ij} = (\\theta_{1,i}^2 + \\epsilon)\\delta_{ij}$ is a diagonal matrix of scaling coefficients, $\\epsilon$ beeing a constant and $\\delta_{ij}$ beeing Dirac-delta.\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2\\exp\\Bigl(-\\sum_{d=1}^D (\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) = \\theta_0^2 \\prod_{d=1}^D \\exp \\Bigl(-(\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) . \n",
    "$$\n",
    "\n",
    "I drop $\\theta$'s from the definition of $K$ for shorthand.\n",
    "\n",
    "Let $\\Sigma$ be the correlation matrix of the training set $\\{(x_i, y_i)\\}_{i=1}^n$ and $k(x)$ a vector with correlations to the new point $x$, to evaluate the functional given by the GP and get the mean at the points $x$  we do:\n",
    "\\begin{align}\n",
    "\\Sigma &:= \\{K(x_i, x_j)\\}_{ij} \\\\\n",
    "k(x) &:= \\{K(x,x_i)\\}_{i=1}^n \\\\\n",
    "f(x) &:= k(x)^t \\Sigma^{-1}y \\\\\n",
    "&= \\sum_{i=1}^n K(x, x_i) \\Sigma_{i}^{-1}y,\n",
    "\\end{align}\n",
    "where $\\Sigma_i^{-1}$ is the $i$'th row of the inverse correlation matrix (of course, we are just solving $\\Sigma z = y$ and take the $i$'th component of $z$, so I will just use $z_i$ as a shorthand:\n",
    "$$ f(x) = \\sum_{i=1}^n z_i K(x, x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use wolframalpha to derive integrands. We also define a shorthand\n",
    "$$\n",
    "c_d := \\sqrt{\\theta_{1d}^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "For the [constant](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28x-y%29%5E2%29dx+from+0+to+1) we have:\n",
    "\\begin{align}\n",
    "f_0 &= \\int_{[0,1]^D} f(x) dx = \\int_{[0,1]^D} \\sum_{i=1}^n z_i K(x, x_i) dx \\\\\n",
    "&= \\sum_{i=1}^n z_i \\theta_0^2 \\prod_{d=1}^D \\int_{0}^{1} \\exp \\Bigl(-c_d^2(x_d-x_{id})^2\\Bigr) dx_d \\\\\n",
    "&= \\theta_0^2 \\sum_{i=1}^n z_i \\prod_{d=1}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d-c_d x_{id})\\Bigr) + \\text{erf}\\Bigl(c_d x_{id}\\Bigr)\\Bigr]}{2c_d}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the [total variance](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28%28x-y%29%5E2%2B%28x-z%29%5E2%29%29dx+from+a+to+b+) we have:\n",
    "\\begin{align}\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D}\\Bigl(f(x) - f_0\\Bigr)^2 dx = \n",
    "\\int_{[0,1]^D} \\Bigl(\\sum_{i=1}^n z_i K(x, x_i) - f_0 \\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j K(x, x_i) K(x, x_j) dx - 2f_0 \\int_{[0,1]^D}\\sum_{i=1}^n z_i K(x, x_i) dx + f_0^2\\int_{[0,1]^D}dx \\\\\n",
    "&= f_0^2\\Bigl(1 - 2\\Bigr) + \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\int_{0}^{1} \\exp\\Bigl(-c_d^2 \\Bigl[(x_d - x_{id})^2 + (x_d - x_{jd})^2\\Bigr]\\Bigr)dx_d \\\\\n",
    "&= \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\frac{\\sqrt{\\frac{\\pi}{2}} \\exp\\Bigl(-1/2c_d^2(x_{id}-x_{jd})^2\\Bigr)\\Bigl[\\text{erf}\\Bigl(\\frac{c_d(x_{id}+x_{jd})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_d(-2 + x_{id} + x_{jd})}{\\sqrt{2}}\\Bigr)\\Bigr]}{2c_d} -f_0^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main effects we have:\n",
    "\\begin{align}\n",
    "f_t(x_t) &= \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr)\\prod_{d\\neq t}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d(1-x_{id})\\Bigr) - \\text{erf}\\Bigl(c_d(-x_{id})\\Bigr)\\Bigr]}{2c_d} - f_0\\\\ \n",
    "&=: \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - f_0 \\\\\n",
    "\\sigma^2(f_t(x_t)) &= \\int_{[0,1]^D} \\Bigl(f_{d}(x_{d})\\Bigr)^2 dx = \\int_{[0,1]^D} \\Bigl(\\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - f_0\\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\exp\\Bigl(-c_t \\Bigl[(x_t - x_{it})^2 + (x_t - x_{jt})^2\\Bigl]\\Bigl) \\\\ \n",
    "&- 2 \\theta_0^2 f_0 \\sum_{i=1}^n z_i P_i \\int_{[0,1]^D} \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) dx + f_0^2 \\int_{[0,1]^D} dx \\\\\n",
    "&= \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\frac{\\sqrt{\\frac{\\pi}{2}} e^{-1/2c_t^2(x_{it}-x_{jt})^2}\\Bigl[\\text{erf}\\Bigl(\\frac{c_t(x_{it}+x_{jt})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_t(-2 + x_{it} + x_{jt})}{\\sqrt{2}}\\Bigr)\\Bigl]}{2c_t} \\\\\n",
    "&- 2\\theta_0^2 f_0 \\sum_{i=1}^n z_i P_i \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_t(1-x_{it})\\Bigr) + \\text{erf}\\Bigl(c_t x_{it}\\Bigr)\\Bigr]}{2c_t}  + f_0^2 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.integrate import dblquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define GP and use normalization\n",
    "dim = 6\n",
    "n_samples = 30\n",
    "theta_0 = 1.0\n",
    "theta_1 = np.array([0.5]*dim)\n",
    "ε = 1e-8\n",
    "c = np.sqrt(theta_1**2 + ε)\n",
    "length_scale = 1.0/(np.sqrt(2)*c)\n",
    "kernel = RBF(length_scale = length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=n_samples, n_features=dim, n_informative=dim, bias=0.5, random_state=1000)\n",
    "\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "X_train = X_norm\n",
    "y_train = y\n",
    "#X_test = X_norm[10:,:]\n",
    "#y_test = y[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = theta_0**2*pairwise_kernels(X_train, metric=kernel)\n",
    "z = np.linalg.solve(K, y_train)\n",
    "\n",
    "def kernel_eval(X):\n",
    "    return theta_0**2*pairwise_kernels(X, X_train, metric=kernel) @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contant $f_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.861314675419521"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant analytic\n",
    "f_0 = theta_0**2 * z.T @ np.prod(np.sqrt(np.pi)/(2*c)* (erf(c*(1-X_train)) + erf(c*X_train)), axis=1)\n",
    "f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 67.468658 rel_error = 6.245860\n",
      "       100 value = -35.770108 rel_error = 1.781217\n",
      "      1000 value = -22.838672 rel_error = 0.775765\n",
      "     10000 value = -12.420848 rel_error = 0.034247\n",
      "    100000 value = -12.856739 rel_error = 0.000356\n",
      "   1000000 value = -12.892767 rel_error = 0.002446\n",
      "  10000000 value = -12.912806 rel_error = 0.004004\n"
     ]
    }
   ],
   "source": [
    "# Constant MC\n",
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=dim*n).reshape((-1,dim))\n",
    "    sol = np.mean(kernel_eval(X))\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs((sol-f_0)/f_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total variance $\\sigma^2(f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_ij(i, j ):\n",
    "    return z[i] * z[j] * np.prod(np.sqrt(np.pi/2)/(2*c)\n",
    "             * np.exp(-0.5*c**2*(X_train[i,:]- X_train[j, :])**2)\n",
    "            * (erf(c/np.sqrt(2)*(X_train[i,:] + X_train[j, :])) \n",
    "              - erf(c/np.sqrt(2)*(X_train[i,:] + X_train[j, :]-2))) )\n",
    "s = 0\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        s += prod_ij(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22498.56999190397"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma2 = theta_0**4* s - f_0**2\n",
    "sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 12760.731040 rel_error = 0.432820\n",
      "       100 value = 26401.269437 rel_error = 0.173464\n",
      "      1000 value = 21461.303211 rel_error = 0.046104\n",
      "     10000 value = 22813.583390 rel_error = 0.014001\n",
      "    100000 value = 22429.778110 rel_error = 0.003058\n",
      "   1000000 value = 22448.529247 rel_error = 0.002224\n",
      "  10000000 value = 22493.184968 rel_error = 0.000239\n"
     ]
    }
   ],
   "source": [
    "# Constant MC\n",
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=dim*n).reshape((-1,dim))\n",
    "    sol = np.mean((kernel_eval(X) - f_0)**2)\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs(sol-sigma2)/sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main effects $f_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_d(i, d): \n",
    "    return np.sqrt(np.pi)/(2*c[d])*(erf(c[d]*(1-X_train[i,d])) + erf(c[d]* X_train[i,d])) \n",
    "\n",
    "def P_i(i, t):\n",
    "    p = 1\n",
    "    for d in range(X_train.shape[1]):\n",
    "        if d != t:\n",
    "            p *= prod_d(i, d)\n",
    "    return p\n",
    "\n",
    "def prod_ijt(i,j,t):\n",
    "    return np.sqrt(np.pi/2)/(2*c[t]) \\\n",
    "             * np.exp(-0.5*c[t]**2*(X_train[i,t]- X_train[j, t])**2) \\\n",
    "            * (erf(c[t]/np.sqrt(2)*(X_train[i,t] + X_train[j, t])) \\\n",
    "              - erf(c[t]/np.sqrt(2)*(X_train[i,t] + X_train[j, t]-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_effect(t):\n",
    "    s1 = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            s1 += z[i]*z[j]*P_i(i, t)*P_i(j, t)*prod_ijt(i, j, t)\n",
    "    s1 *= theta_0**4\n",
    "    s2 = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        s2 += z[i] * prod_d(i, t) * P_i(i, t)\n",
    "    s2 *= -2*theta_0**2*f_0\n",
    "    return s1 + s2 + f_0**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.681206019366385"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me0 = main_effect(0)\n",
    "me0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_t_mc(x, t, n=10**5):\n",
    "    X = np.random.uniform(size=n*dim).reshape((-1,dim))\n",
    "    X[:,t] = x\n",
    "    return np.mean(kernel_eval(X)-f_0)\n",
    "\n",
    "def f_t(x,t):\n",
    "    s = 0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        s += z[i]*np.exp(-c[t]**2*(x - X_train[i, t])**2)*P_i(i, t)\n",
    "    return theta_0**2 * s-f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6621011652429221, -0.6466432623343508)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t_mc(0.3, 0, 10**7), f_t(0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = 4.680114 rel_error = 0.000233\n",
      "       100 value = 3.948890 rel_error = 0.156437\n",
      "      1000 value = 5.064864 rel_error = 0.081957\n",
      "     10000 value = 4.842166 rel_error = 0.034384\n",
      "    100000 value = 4.679914 rel_error = 0.000276\n",
      "   1000000 value = 4.679191 rel_error = 0.000430\n",
      "  10000000 value = 4.685582 rel_error = 0.000935\n"
     ]
    }
   ],
   "source": [
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=n)\n",
    "    sol = np.mean((f_t(X, 0))**2)\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs(sol-me0)/me0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

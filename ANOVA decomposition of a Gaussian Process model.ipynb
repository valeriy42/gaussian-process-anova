{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA docomposition of a general function\n",
    "\n",
    "Functional ANOVA decomposition represents a high-dimensional function as a function of the form\n",
    "$$f(x_1, x_2, \\ldots, x_D) = f_0 + \\sum_{i=1}^D f_i(x_i) + \\sum_{i<j} f_{ij}(x_i, x_j) + \\sum_{i<j<k} f_{ijk}(x_i, x_j, x_k) + \\cdots + f_{1,\\ldots,D}(x_1, \\ldots, x_D).$$\n",
    "\n",
    "\n",
    "Let's first talk about how to compute the ANOVA components in general and then we focus on how to do it for $f(x_1, \\ldots, x_D)$ which is evaluated as a mean of a Gaussian process.\n",
    "\n",
    "Note: without the loss of generality, we assumy that the input is restricted to a hypercube $[0,1]^D$. It will significanlty simplify the notation. We can normalize the input whe computing.\n",
    "\n",
    "The first step is to choose the projection operator $P$:\n",
    "$$Pf := \\int_{[0,1]}f(x)d\\mu(x)$$\n",
    "We are going to use the projection operator $P$ using Lebegue measure $Pf := \\int_{[0,1]} f(x) dx$, so all intergrals should work out as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional case\n",
    "\n",
    "Now we use the projection operator to define the constant and the main effects. We assume $D=2$ for now and then introduce some more notation to generalize:\n",
    "\\begin{align}\n",
    "f_0 & = \\int_{[0, 1]} \\int_{[0,1]} f(x_1, x_2) dx_1 dx_2 \\\\\n",
    "f_1 (x_1) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_2  \\\\\n",
    "f_2 (x_2) & = \\int_{[0, 1]} \\Bigl(f(x_1, x_2) - f_0 \\Bigr) dx_1  \n",
    "\\end{align}\n",
    "\n",
    "The interaction effect $f_{1,2}(x_1,x_2)$ is defined as the remainder to make the ANOVA decomposition to work out correctly:\n",
    "$$f_{1,2}(x_1,x_2) = f(x_1, x_2) - f_0 - f_1(x_1) - f_2(x_2).$$\n",
    "\n",
    "The **total variance** (TV) of the predictor is defined as\n",
    "$$\\sigma^2(f) := \\int_{[0, 1]} \\int_{[0, 1]} (f(x_1, x_2) - f_0)^2 d x_1 d x_2$$\n",
    "\n",
    "One can show that TV is decomposible into the sum of variances of main effects and interactions defined above:\n",
    "\\begin{align}\n",
    "\\sigma_1^2(f_1) &:= \\int_{[0, 1]} (f_1(x_1))^2 d x_1 \\\\\n",
    "\\sigma_2^2(f_2) &:= \\int_{[0, 1]} (f_2(x_2))^2 d x_2 \\\\\n",
    "\\sigma_{1,2}^2(f_{1,2}) &:= \\int_{[0, 1]} \\int_{[0, 1]} \\left(f_{1,2}(x_1, x_2) - f_0 - \\left[f_1(x_1)-f_0\\right] - \\left[f_2(x_2)-f_0\\right]\\right)^2 dx_1 dx_2 \\\\ \n",
    "\\sigma^2(f) &= \\sigma_1^2(f_1) + \\sigma_2^2(f_2) + \\sigma_{1,2}^2 (f_{1,2}).\n",
    "\\end{align}\n",
    "\n",
    "And so, by dividing individual variances by TV we can express these compoents as percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General case\n",
    "\n",
    "Using subsets $u\\subseteq \\{1,\\ldots, D\\}$, we can establish a shorthand notation for ANOVA components, where $f_u$ and $x_u$ represents a subset of vector $x$ with components $x_i, i \\in u$. The we have\n",
    "\\begin{align}\n",
    "f(x_1, \\ldots, x_D) &= \\sum_{u \\subseteq\\{1\\ldots,D\\}} f_{u}(x_{u}),\\\\\n",
    "f_{u}(x_{u}) &= \\int_{[0,1]^{D-|u|}} \\Bigl(f(x) - \\sum_{v\\subsetneq u}f_v(x_v)\\Bigr)dx_{-u}\\\\\n",
    "\\sigma^2(f_u) &= \\int_{[0,1]^D} \\Bigl(f_u(x_u)\\Bigr)^2dx \\\\\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D} \\Bigl(f(x) - f_0\\Bigr)^2 dx = \\sum_{u \\subseteq \\{1,\\ldots,D\\}, u\\neq\\emptyset}\\sigma^2(f_u).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA for Gaussian process\n",
    "\n",
    "For Bayesian optimization we are using the Gaussian proces defined by a parametrized RBF kernel of the form\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2 \\exp\\Bigl(-(x-x')^t D(\\vec{\\theta}_1) (x-x'))\\Bigr),\n",
    "$$\n",
    "where $[D(\\vec{\\theta}_1)]_{ij} = (\\theta_{1,i}^2 + \\epsilon)\\delta_{ij}$ is a diagonal matrix of scaling coefficients, $\\epsilon$ beeing a constant and $\\delta_{ij}$ beeing Dirac-delta.\n",
    "$$\n",
    "K(x,x'|\\theta_0, \\vec{\\theta}_1) = \\theta_0^2\\exp\\Bigl(-\\sum_{d=1}^D (\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) = \\theta_0^2 \\prod_{d=1}^D \\exp \\Bigl(-(\\theta_{1d}^2+\\epsilon)(x_d-x'_d)^2\\Bigr) . \n",
    "$$\n",
    "\n",
    "I drop $\\theta$'s from the definition of $K$ for shorthand.\n",
    "\n",
    "Let $\\Sigma$ be the correlation matrix of the training set $\\{(x_i, y_i)\\}_{i=1}^n$ and $k(x)$ a vector with correlations to the new point $x$, to evaluate the functional given by the GP and get the mean at the points $x$  we do:\n",
    "\\begin{align}\n",
    "\\Sigma &:= \\{K(x_i, x_j)\\}_{ij} \\\\\n",
    "k(x) &:= \\{K(x,x_i)\\}_{i=1}^n \\\\\n",
    "f(x) &:= k(x)^t \\Sigma^{-1}y \\\\\n",
    "&= \\sum_{i=1}^n K(x, x_i) \\Sigma_{i}^{-1}y,\n",
    "\\end{align}\n",
    "where $\\Sigma_i^{-1}$ is the $i$'th row of the inverse correlation matrix (of course, we are just solving $\\Sigma z = y$ and take the $i$'th component of $z$, so I will just use $z_i$ as a shorthand:\n",
    "$$ f(x) = \\sum_{i=1}^n z_i K(x, x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use wolframalpha to derive integrands. We also define a shorthand\n",
    "$$\n",
    "c_d := \\sqrt{\\theta_{1d}^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "For the [constant](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28x-y%29%5E2%29dx+from+0+to+1) we have:\n",
    "\\begin{align}\n",
    "f_0 &= \\int_{[0,1]^D} f(x) dx = \\int_{[0,1]^D} \\sum_{i=1}^n z_i K(x, x_i) dx \\\\\n",
    "&= \\sum_{i=1}^n z_i \\theta_0^2 \\prod_{d=1}^D \\int_{0}^{1} \\exp \\Bigl(-c_d^2(x_d-x_{id})^2\\Bigr) dx_d \\\\\n",
    "&= \\theta_0^2 \\sum_{i=1}^n z_i \\prod_{d=1}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d-c_d x_{id})\\Bigr) + \\text{erf}\\Bigl(c_d x_{id}\\Bigr)\\Bigr]}{2c_d}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the [total variance](https://www.wolframalpha.com/input/?i=integral+exp%28-%28c%5E2%29%28%28x-y%29%5E2%2B%28x-z%29%5E2%29%29dx+from+a+to+b+) we have:\n",
    "\\begin{align}\n",
    "\\sigma^2(f) &= \\int_{[0,1]^D}\\Bigl(f(x) - f_0\\Bigr)^2 dx = \n",
    "\\int_{[0,1]^D} \\Bigl(\\sum_{i=1}^n z_i K(x, x_i) - f_0 \\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j K(x, x_i) K(x, x_j) dx - 2f_0 \\int_{[0,1]^D}\\sum_{i=1}^n z_i K(x, x_i) dx + f_0^2\\int_{[0,1]^D}dx \\\\\n",
    "&= f_0^2\\Bigl(1 - 2\\Bigr) + \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\int_{0}^{1} \\exp\\Bigl(-c_d^2 \\Bigl[(x_d - x_{id})^2 + (x_d - x_{jd})^2\\Bigr]\\Bigr)dx_d \\\\\n",
    "&= f_0^2(\\ldots) + \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j \\prod_{d=1}^D \\frac{\\sqrt{\\frac{\\pi}{2}} \\exp\\Bigl(-1/2c_d^2(x_{id}-x_{jd})^2\\Bigr)\\Bigl[\\text{erf}\\Bigl(\\frac{c_d(x_{id}+x_{jd})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_d(-2 + x_{id} + x_{jd})}{\\sqrt{2}}\\Bigr)\\Bigr]}{2c_d}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main effects we have:\n",
    "\\begin{align}\n",
    "f_t(x_t) &= \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr)\\prod_{d\\neq t}^D \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_d(1-x_{id})\\Bigr) - \\text{erf}\\Bigl(c_d(-x_{id})\\Bigr)\\Bigr]}{2c_d} - f_0\\\\ \n",
    "&=: \\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - F_0 \\\\\n",
    "\\sigma^2(f_t(x_t)) &= \\int_{[0,1]^D} \\Bigl(f_{d}(x_{d})\\Bigr)^2 dx = \\int_{[0,1]^D} \\Bigl(\\theta_0^2 \\sum_{i=1}^n z_i \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) P_i - F_0\\Bigr)^2 dx \\\\\n",
    "&= \\int_{[0,1]^D} \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\exp\\Bigl(-c_t \\Bigl[(x_t - x_{it})^2 + (x_t - x_{jt})^2\\Bigl]\\Bigl) \\\\ \n",
    "&- 2 \\theta_0^2 F_0 \\sum_{i=1}^n z_i P_i \\int_{[0,1]^D} \\exp\\Bigl(-c_t^2(x_t-x_{it})^2\\Bigr) dx + F_0^2 \\int_{[0,1]^D} dx \\\\\n",
    "&= \\theta_0^4 \\sum_{i=1}^n \\sum_{j=1}^n z_i z_j P_i P_j \\frac{\\sqrt{\\frac{\\pi}{2}} e^{-1/2c_t^2(x_{it}-x_{jt})^2}\\Bigl[\\text{erf}\\Bigl(\\frac{c_t(x_{it}+x_{jt})}{\\sqrt{2}}\\Bigr)-\\text{erf}\\Bigl(\\frac{c_t(-2 + x_{it} + x_{jt})}{\\sqrt{2}}\\Bigr)\\Bigl]}{2c_t} \\\\\n",
    "&- 2\\theta_0^2 F_0 \\sum_{i=1}^n z_i P_i \\frac{\\sqrt{\\pi}\\Bigl[\\text{erf}\\Bigl(c_t(1-x_{it})\\Bigr) - \\text{erf}\\Bigl(c_t(0-x_{it})\\Bigr)\\Bigr]}{2c_t}  + F_0^2 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "from scipy.integrate import dblquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define GP and use normalization\n",
    "dim = 2\n",
    "theta_0 = 0.3\n",
    "theta_1 = np.array([2.0]*dim)\n",
    "ε = 1e-4\n",
    "c = np.sqrt(theta_1**2 + ε)\n",
    "kernel_test = lambda x, y: theta_0 * np.exp(-(x-y)**2@c**2)\n",
    "length_scale = 1.0/(np.sqrt(2)*c)\n",
    "kernel = RBF(length_scale = length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=20, n_features=dim, n_informative=dim, bias=0.5, random_state=1000)\n",
    "\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "X_train = X_norm[:10, :]\n",
    "y_train = y[:10]\n",
    "X_test = X_norm[10:,:]\n",
    "y_test = y[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = theta_0**2*pairwise_kernels(X_train, metric=kernel)\n",
    "z = np.linalg.solve(K, y_train)\n",
    "\n",
    "def kernel_eval(X):\n",
    "    return theta_0**2*pairwise_kernels(X, X_train, metric=kernel) @ z\n",
    "\n",
    "K_test = pairwise_kernels(X_train, metric=kernel_test)\n",
    "z_test = np.linalg.solve(K_test, y_train)\n",
    "def kernel_test_eval(X):\n",
    "    return pairwise_kernels(X, X_train, metric=kernel_test) @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-61.30361206 -57.98808062  22.53900295 -24.28231192 -16.72602774\n",
      "  -2.12929409 -41.86531635  44.5630799   41.67531984 -12.97457399]\n",
      "[-61.30361206 -57.98808062  22.53900295 -24.28231192 -16.72602774\n",
      "  -2.12929409 -41.86531635  44.5630799   41.67531984 -12.97457399]\n",
      "[-204.34537353 -193.29360207   75.13000982  -80.94103974  -55.7534258\n",
      "   -7.09764696 -139.55105451  148.54359967  138.9177328   -43.24857997]\n"
     ]
    }
   ],
   "source": [
    "y_eval = theta_0**2 * pairwise_kernels(X_test, X_train, metric=kernel) @ z\n",
    "print(y_eval)\n",
    "print(kernel_eval(X_test))\n",
    "print(kernel_test_eval(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91226262, 0.97676297, 1.35994095, 1.25343965, 1.52405704,\n",
       "       1.51234951, 1.08494107, 1.18481574, 0.67870414, 1.37359053])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(np.sqrt(np.pi)/(2*c)* erf(c*(1-X_train)) + erf(c*X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74524907, 0.57208312],\n",
       "       [0.64781443, 0.60612862],\n",
       "       [0.74680421, 0.72391626],\n",
       "       [0.74265738, 0.70187627],\n",
       "       [0.7429866 , 0.74143716],\n",
       "       [0.74660759, 0.73705457],\n",
       "       [0.67494038, 0.54701156],\n",
       "       [0.67883566, 0.74305994],\n",
       "       [0.62949642, 0.50133195],\n",
       "       [0.70611856, 0.5676102 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.pi)/(2*c)* (erf(c*(1-X_train)) + erf(c*X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contant $f_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1358132079939054"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant analytic\n",
    "f_0 = theta_0**2 * z.T @ np.prod(np.sqrt(np.pi)/(2*c)* (erf(c*(1-X_train)) + erf(c*X_train)), axis=1)\n",
    "f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        10 value = -12.693817 rel_error = -4.943318\n",
      "       100 value = 4.239582 rel_error = -2.984997\n",
      "      1000 value = -5.003225 rel_error = -1.342539\n",
      "     10000 value = -1.203545 rel_error = -0.436493\n",
      "    100000 value = -2.029017 rel_error = -0.050002\n",
      "   1000000 value = -2.113715 rel_error = -0.010347\n",
      "  10000000 value = -2.142167 rel_error = -0.002975\n"
     ]
    }
   ],
   "source": [
    "# Constant MC\n",
    "for n in 10**np.array([1,2,3,4,5, 6, 7]):\n",
    "    X = np.random.uniform(size=2*n).reshape((-1,dim))\n",
    "    sol = np.mean(kernel_eval(X))\n",
    "    print('%10d value = %.6f rel_error = %.6f' % (n, sol, np.abs(sol-f_0)/f_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total variance $\\sigma^2(f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.pi/2)/(2*c)*pairwise_kernels(X_train, metric=RBF(length_scale=1/c)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = f_0**2 + theta_0^2*z.T @ z.T @ np.prod()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
